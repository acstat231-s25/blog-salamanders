---
title: "Data Wrangling"
format: html
editor: visual
---

```{r}
#| label: set-up
#| include: false

knitr::opts_chunk$set(
  tidy=FALSE,     # display code as typed
  size="small",   # slightly smaller font for code
  message=FALSE)   

# load packages here
library(rvest)
library(tidyverse)
library(kableExtra)
library(mdsr)
library(stringr)
library(robotstxt)
library(tidytext)
library(dplyr)
```

```{r}
# check permissions ----------------------------------------------------------
songs_url <- "https://cs.uwaterloo.ca/~dtompkin/music/list/Best9.html"
paths_allowed(songs_url)
# scrape the table -----------------------------------------------------------
songs <- songs_url |>
  read_html() |>
  html_elements("table") |>
  purrr::pluck(2) |>
  html_table()  

```

```{r}
#| label: cleanup table

songs_clean <- songs |> 
  select(-X2,-X10) |>
  rename(number = X1, artist = X3, title = X4, time = X5
         , bpm = X6, year = X7 , genre = X8, track = X9) |>
   filter(artist != "ARTIST") |>
  mutate(track_gen = str_remove(track, "-.."))
```

```{r}
# create vectors compatible with paste0
tracks <- as.vector(unlist(songs_clean['track']))
tracks_gens <- as.vector(unlist(songs_clean['track_gen']))
titles <- as.vector(unlist(songs_clean['title']))

# build urls
urls2 <- paste0("https://cs.uwaterloo.ca/~dtompkin/music/track/"
                , tracks_gens, "/", tracks, ".html")
head(urls2)
```

```{r}

# Identify number of iterations (start with 1!)
n_iter <- 250
# Pre-allocate space
song_lyrics1 <- tibble(title = titles, url = urls2) |>
  # Limit to only the rows we are trying to work with
  slice(1:n_iter)

lyrics_df <- tibble(# Create empty character vector the same
  # length as our titles vector
  lyrics = vector(mode = "character", length = length(titles))) |>
  slice(1:n_iter) 
# Iterate through links to grab nursery rhyme text
for (i in 1:n_iter) {
  # Scrape song i's lyric table and save it to row i of the `text` variable
  #tables <- song_lyrics1$url[i] |>
  #  read_html() |>
  #  html_elements("table") |>
  #  purrr::pluck(6) |>
  #  html_table()
  # extract cell of table with lyrics
  #lyrics_df$lyrics[i] <- tables[[3, 1]] |>
  #  str_replace_all("[\r\n]+", " ")
  
  lyrics_df$lyrics[i] <- song_lyrics1$url[i] |>
  read_html() |>
  html_elements("table") |>
  purrr::pluck(6) |>
  html_element(xpath = ".//tr[3]/td[1]") |>
  html_text2() |>
  str_replace_all("[\\n\\r]+", " ") |>  # Replace line breaks with a space
  str_squish()
  
}
# combine the two datafiles
song_lyrics1 <- bind_cols(song_lyrics1, lyrics_df)

# merge song_lyrics with songs_clean to add `url` and `lyrics` to songs_clean
merged_songs <- songs_clean |>
  left_join(song_lyrics1 |> select(title, url, lyrics), by = "title")
```

```{r}
# create df for cumulative songs dataset
#
#read_csv("blog-salamanders/data/merged_songs.csv")

# create bigrams dataset for lyrics
 merged_songs_bigrams <- merged_songs |>
    unnest_tokens(output = bigram, input = lyrics, token = "ngrams", n = 2) |>
   select(number, artist, title, bigram)
 
 # create trigrams dataset for lyrics
 merged_songs_trigrams <- merged_songs |>
   unnest_tokens(output = trigram, input = lyrics, token = "ngrams", n = 3) |>
   select(number, artist, title, trigram)
 
 # df for word frequencies for word clouds
 lyrics_word_freqs <- merged_songs |>
    unnest_tokens(output = word, input = lyrics) |>
    anti_join(stop_words, by = "word") |>
    count(word, sort = TRUE)

```

```{r}
#write_csv(merged_songs, "blog-salamanders/data/merged_songs.csv")
write_csv(merged_songs, "data/merged_songs.csv")
read_csv("data/merged_songs.csv")
```

```{r}

afinn_lexicon = get_sentiments("afinn")

words_by_year = merged_songs |>
  unnest_tokens(output = word, input = lyrics, token = "ngrams", n = 1)

matched_words = inner_join(afinn_lexicon, words_by_year) |>
  mutate(year = as.numeric(year)) |>
  group_by(year, ) |>
  summarize(average_sentiment_yearly = mean(value, na.rm = TRUE)) |>
  arrange(year)

```

```{r}

ggplot(matched_words, aes(x = year, y = average_sentiment_yearly)) +
  geom_point() +
  geom_line(linewidth = 0.75, alpha = 0.6) +
  scale_x_continuous(breaks = seq(min(matched_words$year), max(matched_words$year), by = 2)) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
```
